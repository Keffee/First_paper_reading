# 总结一下近期的大模型辅助推荐的工作，做个ppt
- NIPS '23, [Recommender Systems with Generative Retrieval](https://arxiv.org/pdf/2305.05065.pdf)(TIGER)(Google)
  - 简略版：
    - 给定一个item的文本描述，使用预训练的文本编码器生成dense的embedding，然后量化生成语义ID，再根据语义ID反推出语义ID对应的embedding，经过平均池化就能得到每个语义ID的embedding了
    - 之后使用这种语义ID（也就是说embedding皆已固定）进行推荐，接下来构造负例，分别是把ID半修改生成的负例、把同batch的其他预测ID作为负例；通过置换矩阵使得多个域能对齐
  - 重看版：
    - 避免矩阵分解范式，直接预测候选id的端到端生成范式，尾部是一个生成式检索模型
      - (WWW23)VQ-Rec建立可转移的推荐系统而非检索
        - VQ-Rec认为item rep.和text rep.过于紧密，问题如下：
          - 过分强调文本相似性，忽略序列性
          - 文本的不同表述可能导致在语义空间的映射不同
        - 目的：预训练一个可转移的序列推荐器
        - 转移中，训练一个转移矩阵，这个转移矩阵基于Birkhoff-von Neumann定理进行优化，用来转移码元嵌入
      - 生成检索来自于文档检索
    - 使用文本编码器为item生成embedding，然后使用量化获得语义ID，优点有3：
      - 基于语义的ID在相似item上天然具有相似性
      - 避免内在反馈循环(inherent feedback loop)，推荐系统会倾向于忽略长尾节点
      - 减轻存储压力
    - 在生成语义ID之后，直接将语义ID送入seq2seq模型进行训练，例如，i_1=(3,2,4), i_2=(7,4,1), i_3=(2,1,5)，则输入(3,2,4,7,4,1,2,1,5)，来预测接下来三个id
- WWW '23, [Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders](https://arxiv.org/pdf/2210.12316.pdf)(VQ-Rec)(人大)
  - 给定文本信息，利用RQ-VAE技术，将文本编码并量化为语义ID；由于RQ-VAE存在编码坍塌的问题，考虑使用k-means和VQ-VAE进行编码；碰撞则额外加一维进行编码
  - 直接训练一个序列到序列模型，生成预测item的语义ID，实验中使用T5X模型
  - 实验发现生成无效ID的数量不大，而且效果很好（效率应该不高）
  - 重看：
    - VQ-Rec认为item rep.和text rep.过于紧密，问题如下：
      - 过分强调文本相似性，忽略序列性
      - 文本的不同表述可能导致在语义空间的映射不同
    - 目的：预训练一个可转移的序列推荐器
    - 转移中，训练一个转移矩阵，这个转移矩阵基于Birkhoff-von Neumann定理进行优化，用来转移码元嵌入
- arXiv '23, [A Multi-facet Paradigm to Bridge Large Language Model and Recommendation](https://arxiv.org/pdf/2310.06491)(NUS)(TransRec)
  - 局限：
    - ID标识对大模型而言很难理解，需要大量微调
    - 语义标识缺乏区分度，同时描述相似的item未必有相似的交互
  - 分别构建ID、title、attribute指令数据集
  - 使用FM索引来生成有效token，在每个指令集内，利用生成对应token的概率计算得分，在指令集之间，利用超参数和相似度计算总得分
  - ID和title最为关键，attribute和FM索引法相对影响较小一些
- arXiv '24, [Knowledge adaptation from large language model to recommendation for practical industrial application](https://arxiv.org/pdf/2405.03988)(Kuaishou)
  - 现有问题：
    - 推荐数据量大，LLM推理速度慢、输入长度有限
    - 灾难性遗忘和性能问题
  - 挑战：
    - 领域适配微调过程中导致原始性能下降
    - 预训练目标不匹配
  - 工作：将知识从LLM适配到推荐
    - 双塔结构，嵌入模块&理解模块
    - item进行句子描述之后，对token的输出进行平均池化得到嵌入（效果好于最后一个item的嵌入）
    - 对比学习：固定ui，同u不同i为正例，不同u的交互为负例
  - 实验结果
    - 相比id嵌入和BERT嵌入，LLM嵌入效果更好
    - 冻结大模型做编码好于微调大模型编码好于冻结大模型生成
    - 发现在H@10和N@10上效果不如HSTU，但是H@50和N@50上更好
    - 单向masked attention超过了自注意力超过了什么都不做

- arXiv '24, [CALRec: Contrastive Alignment of Generative LLMs For Sequential Recommendation](https://arxiv.org/pdf/2405.02429)(google)
  - 两阶段大模型微调方法，双塔模型，基于文本进行推荐
    - 纯文本输入，提示工程设计
    - 混合训练目标，包括自定义next item生成和辅助的对比任务
    - 混合finetune+特定任务finetune
    - BM25商品检索方法
  - 任务看作一个seq2seq生成式任务，类似TALLRec，输入是user交互序列，输出是next_item文本，然后作交叉熵损失
  - LLM对user history的平均池化embedding 对比学习 LLM对target item的平均池化embedding
  - LLM对next item的平均池化embedding 对比学习 LLM对target item的平均池化embedding
  - 推理的时候，优先根据大模型生成分数进行排名，然后对各个位置用BM25找到最近item
  - 训练
    - 使用amazon的14个产品类别，全部用于1阶段微调，然后将其中6个进行类别特定微调
    - 发现amazon存在部分重复记录，可能导致推荐性能不佳
  - 实验结果
    - 发现不需要BM25也可以有较为精确的匹配，实际上超过99.9%的输出符合标准，标题有95-99.8%可以直接匹配到
    - 实验发现微调带来了巨大收益（基于PaLM-2），其次两个阶段的微调重要程度基本相同
- WWW '24, [Enhancing Sequential Recommendation via LLM-basedSemantic Embedding Learning](https://dl.acm.org/doi/pdf/10.1145/3589335.3648307)(SAID)(浙江大学)(ali)(阿里)(支付宝)
  - 现有问题：
    - 大模型提取的文本表征相对粒度较粗，无法捕获细粒度
    - 大模型受到长度限制
  - 模型比较简单
    - stage1用mlp来将item id投影到LLM中
    - stage2用这个mlp来作为embedding输入seq模型中
  - 结果可观
    - 相比提取llm最后一层，这种方式训练的效果更好
    - 可以将这种训练范式看作一种embedding初始化，从这个视角看的话，可以发现其性能超过了传统的初始化方法。同时，后续继续进行训练能够将效果进一步提升。
- WWW '24, [Aligned Side Information Fusion Method for SequentialRecommendation](https://dl.acm.org/doi/pdf/10.1145/3589335.3648308)(ant)(蚂蚁)
  - 探索一种id信息和辅助信息混合的方式，总结来看方法较为复杂
- WWW '24, [Context-based Fast Recommendation Strategy for Long User Behavior Sequence in Meituan Waimai](https://arxiv.org/pdf/2403.12566)(CoFARS)(meituan)(美团)
  - 识别出与目标上下文具有相似用户偏好的上下文，从而找到响应兴趣点，解决超长序列问题
  - 
- arXiv '24, [Rethinking Large Language Model Architectures for Sequential Recommendations](https://arxiv.org/pdf/2402.09543)(LITE-LLM4Rec)
  - 问题
    - LLM4Rec计算开销大
    - 束搜索解码非常耗时
    - token化输入引入了过多的计算
  - 方法
    - 上下文感知嵌入：item信息输入大模型，表征做平均池化
    - 序列表示获取：大模型继续编码之前获得的所有item表征，随后使用平均池化获得user表征
    - user表征过MLP得到对物品集的输出得分（提高速度，避免大模型的冗余计算）
- SIGIR '24, [UniSAR: Modeling User Transition Behaviors between Search and Recommendation](https://arxiv.org/pdf/2404.09520)(UniSAR)(思考下更多可能)(Kuaishou)
  - 搜推融合：提取 对齐 融合
  - 用户会出现搜和推的转换，因此会出现留存/转换，共4种操作
  - 具体方法：
    - 嵌入模块
      - 采用三个嵌入矩阵，user，item和查询query
      - 查询嵌入直接将查询词的嵌入平均池化，之后使用id embedding，如果是搜索嵌入则直接加上查询嵌入
      - e_q（搜索中的item嵌入）和e_i（推荐中的item交互）做对比学习
    - 转换模块
      - 取出s子序列和r子序列做出嵌入，加入位置编码，于子历史序列内部进行多头自注意力编码
      - 在完整序列中加入掩码，让注意力交互只在r-s之间进行，不在rr和ss内进行
    - 对齐模块
      - 将s2s/r2r和r2s分别对比学习，loss相加
    - 融合模块
      - 首先由s2s和r2r做msa，然后将r2s做q，msa结果做kv，作为item最终表示
      - 再通过与目标项的注意力计算来得到搜和推的历史聚合表示
      - 历史交互拼成序列交给MMOE进行预测
  - 实验结果
    - 从实验结果看，似乎r2s和s2r的作用并不算太大？
- arXiv '24, [Learnable Tokenizer for LLM-based Generative Recommendation](https://arxiv.org/pdf/2405.07314)(LETTER)(NUS)(冯福利)
  - 哈哈，和我的想法基本一样
  - 使用对比学习进行CF-大模型表征拉近
  - 类似tiger的生成式推荐方法
