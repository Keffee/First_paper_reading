<!-- 接下来的内容是各个开会、讨论的记录 -->
#### 20230317 美团进度
- 缺少公开数据集
- \*没想到过去的实习生，也不完全跟着实验走，也要自己做research
- 想法：右边是搜索，左边两个行为数据是行为数据（推荐），
- 最近去调研一下最近的搜推融合方法（包括冠琪师兄发在群里的内容）
- 论文列表：
    1. Knowledge Enhanced Personalized Search（知识图谱）
    1. Keyword-Based Knowledge Graph Exploration Based on Quadratic Group Steiner Trees（在复杂知识图谱上的搜索）
    1. FedPS: A Privacy Protection Enhanced Personalized Search Framework（个性化搜索中的隐私泄露问题）
    1. Attentive Long Short-Term Preference Modeling for 5. Personalized Product Search（用长短期记忆网络辅助个性化搜索）
    1. Embedding-based Retrieval in Facebook Search（facebook上的个性化搜索）
    1. Modeling User Behavior with Graph Convolution for Personalized Product Search（对用户连续行为图进行建模，挖掘用户偏好（可这和搜推有什么关系呢？）
    1. CL4CTR: A Contrastive Learning Framework for CTR Prediction（进行特征表示工程）
    1. Efficient and effective training of language and graph neural network models（GNN结合大规模语言模型做推荐，先看看吧）
    1. ReprBERT: Distilling BERT to an Efficient Representation-Based Relevance Model for E-Commerce（接下来的四篇都看过了，都旨在解决搜索结果与用户意图不匹配的问题）
    1. Graph-based Weakly Supervised Framework for Semantic Relevance Learning in E-commerce
    1. Learning a Product Relevance Model from Click-Through Data in E-Commerce
    1. Weakly Supervised Co-Training of Query Rewriting and Semantic Matching for e-Commerce

#### 20230324 组会
- 回去看看instructive few-shot的那篇
- 回去看看tiger
- 两篇复现一下
- 研究一下冠琪师兄的蒸馏方法，看看和transformer+gnn有什么关系
- autodl——线上服务器

#### 20230407 组会
- 目前师兄都在跑代码，尝试复现各自的论文（并且调试）
- 一种思路的转变->转向生成式网络

#### 20230411 讨论
- 思考搜推融合能不能继续下去
- 思考prompt方法能不能融进来，比如永强师兄的框架能不能直接把prompt部分改成输入搜索query的方式

#### 20230418 会议记录
-（回去看一下HGCL与其他人的对比，看看别人的NDCG是不是真有那么高）（此条删除）
- 尹铭佳
    - 工作规划
        - 使用预训练的多行为长序列建模
            - 图增强的序列推荐
            - 长序列问题优化效率问题
            - 多行为长序列问题
    - 已有工作
        - [SRGNN](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1811.00855), [GCSAN](https://www.ijcai.org/Proceedings/2019/0547.pdf), [GCEGNN](https://dl.acm.org/doi/pdf/10.1145/3397271.3401142)
    - 已有实验结果：
        - 数据集：ML-M, Amazon-beauty, Diginetica, Gowalla
        - 模型：GRU4Rec, SASRec(BCE), SRGNN, GCSAN, GCEGNN
        - SASRec+图对比学习：对于稀疏数据集有着较好提升，但是对稠密矩阵的效果与单纯使用SASRec无明显提升
            - **提问**：取了几个负样本？**回答**：只用了一个
        - SASRec+DirectAU: 在稀疏数据集上产生了较大提升，MF+Di的效果远小于SASRec+DirectAU，可能是因为需要引入更复杂的模型才能支持
            - **提问**：是否应该增加采样，因为现在此领域已经有很多相关结论，应当尝试更多负样本的情况
- 徐翔
    - 超长序列召回：
        - [阿里SIM](https://arxiv.org/pdf/2006.05639.pdf)，soft search效果更好，但是效率较差，因此使用hard search，效果并没有差很多，但是效率好了，可以正常上线
        - [华为UBR](https://arxiv.org/pdf/2005.14171.pdf)
        - [阿里ETA(End-to-End User Behavior Retrieval)](https://arxiv.org/abs/2108.04468)，这是一个CTR模型，更新频率更高，训练/推理成本更低，端到端，
        - [美团SDIM(Sampling-based-Deep Interest Modeling)](https://arxiv.org/pdf/2205.10249.pdf)，使用哈希指纹，在提高AUC的同时也达到了非常高的效率
        - [ADFM(Adversarial Filtering Modeling on Long-term User Behavior Sequences for Click-Through Rate Prediction)](https://arxiv.org/abs/2204.11587)，
            - **提问**：对这样的长序列，是如何处理里面的每个item的？
            **回答**：聚类处理用户的behavior，去除冗余item；
            - **提问**：BSU框架中为何先进行分桶后还要再对user behavior重新top-k分组？这样的处理有意义吗？
            **回答**：留待之后详细研究
        - [快手TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou](https://arxiv.org/pdf/2302.02352.pdf)，解决目标不一致问题
        - Efficient Dense Retrieval
- 郭威
    - 长序列、多行为进展：
        - 端到端：
            - 长序列：[UBR](https://arxiv.org/pdf/2005.14171.pdf)、[SIM]((https://arxiv.org/pdf/2006.05639.pdf))、[ETA](https://arxiv.org/abs/2108.04468)、[SDIM](https://arxiv.org/pdf/2205.10249.pdf)、ETA+ 
            - 多行为：MBSTR，HPMR
        - **提问**：目前端到端效果如何？
        **回答**：沿着逐步开发的路线，UBR->SIM->ETA，发现端到端的ETA效果会更好一些，更能捕捉全部item的特征。
        - 预训练：word2vec，Transformer，GNN；需要一些新颖的东西来进行提升
        - 下游如何微调

#### 20230425 旁听北京航天局参观实验室
- 北京航天自动控制研究所简介
    - 控制航天运载工具的运行之类的
    - 宇航智能控制技术全国重点实验室
        - 高速飞行的制导控制方法：飞得稳，投得准
        - 精确制导：最末端的制导
        - 光学制导：合作下，非合作下、对抗下
        - 控制系统一体化：设备轻小型、国产化、全自主
        - 快速发射

#### 20230505 组会
- 看看每年的best paper，提升提升？

#### 20230510 组会
- query到click有一个噪声变量，铭佳师兄有一篇就是这个方法，
- 17号把所有结果都跑通
- lightgcn，bpr
- 一个问题：目前的负采样由于负采样时采到正样本的概率很低，故而没有考虑去除负采样中的正样本的操作（因为太慢了），但是最后表述的时候应该要注意一下

#### 20230517 组会
- 

#### 20230524 组会
- 看看ICLR吧，比较不水（WWW, KDD, CVPR）
- 日后投稿组内要先过一遍
- 以后读一下因果x多行为（如果可能的话看一看CoT）
- 现在集中在可信上：可解释、鲁棒、安全

### 20230614 组会
- 所有人40分钟快速过一下个人工作
- 我跟永强师兄一组，一共20分钟，轮流进行：一个人详细讲一个人少讲
- 然后talk的那个人再讲40分钟